{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d029ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import torchvision.transforms as transforms\n",
    "import clip\n",
    "from transformers.models.clip import CLIPTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "from utils import train_val_test_split\n",
    "\n",
    "from mimic_dataset import MIMIC_DataSet\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from timm.models.vision_transformer import Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "227bb022",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "\n",
    "        # flatten: [B, C, H, W] -> [B, C, HW]\n",
    "        # transpose: [B, C, HW] -> [B, HW, C]\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "    \n",
    "def zero_padding(text_tensor, tar_dim, device=None):\n",
    "    padding_size = tar_dim - text_tensor.shape[1]\n",
    "    zero_tensor = torch.zeros((text_tensor.shape[0], padding_size), device=device)\n",
    "    padded_tensor = torch.cat([text_tensor, zero_tensor], dim=1)\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90bab75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self,modality,dim):\n",
    "        super().__init__()\n",
    "        self.modality = modality\n",
    "        self.embed_dim = dim\n",
    "        if self.modality == 'image' or self.modality == 'infrared' or self.modality == 'x-ray':\n",
    "            self.embed = PatchEmbed(embed_dim=self.embed_dim)\n",
    "        elif self.modality == 'text':\n",
    "            self.embed = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", truncation=True)\n",
    "\n",
    "    def forward(self,data):\n",
    "        if self.modality in ['image', 'text' ]:\n",
    "            embeddings = self.embed(data)\n",
    "        elif self.modality =='text':\n",
    "            embeddings = self.embed(data)\n",
    "            embeddings = zero_padding(text_tensor=embeddings, tar_dim = self.embed_dim)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892783e0",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "620650c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/fe/baur/datasets/mimic-cxr-jpg-2.0.0-small/'\n",
    "\n",
    "label_file = pd.read_csv(data_path + \"mimic-cxr-2.0.0-chexpert.csv\")\n",
    "\n",
    "train, test_val = train_val_test_split(label_file, 0.2)\n",
    "val, test = train_val_test_split(test_val, 0.5)\n",
    "\n",
    "mean = 0.4992\n",
    "std = 0.2600\n",
    "\n",
    "train_transform = A.Compose([A.Resize(256, 256, always_apply=True),\n",
    "                                 A.CenterCrop(224, 224, always_apply=True),\n",
    "                                 A.Normalize(mean=mean, std=std),\n",
    "                                 ToTensorV2()])\n",
    "\n",
    "mimic_train = MIMIC_DataSet(data_path, train, train_transform, 'multilabel', 'PA', tokenize=False)\n",
    "mimic_val = MIMIC_DataSet(data_path, train, train_transform, 'multilabel', 'PA', tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82fa655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d22aeabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = mimic_train\n",
    "                              , batch_size = 8\n",
    "                              , shuffle=False\n",
    "                              , num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b22ff7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f2283dc9010>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e507471c",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6264fab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ckpt = torch.load('/home/fe/baur/Downloads/Meta-Transformer_base_patch16_encoder (1).pth')\n",
    "encoder = nn.Sequential(*[\n",
    "            Block(\n",
    "                dim=768,\n",
    "                num_heads=12,\n",
    "                mlp_ratio=4.,\n",
    "                qkv_bias=True,\n",
    "                norm_layer=nn.LayerNorm,\n",
    "                act_layer=nn.GELU\n",
    "            )\n",
    "            for i in range(12)])\n",
    "encoder.load_state_dict(ckpt,strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d5f9f9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (1): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (2): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (3): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (4): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (5): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (6): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (7): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (8): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (9): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (10): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (11): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9206b9f",
   "metadata": {},
   "source": [
    "# Image Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81992097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data2Seq(\n",
       "  (embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_tokenizer_img = Data2Seq(modality='image',dim=768)\n",
    "\n",
    "auto_tokenizer_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fe8cb76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(auto_tokenizer_img(x[0]).shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5203ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load('ViT-B/32', 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "816be6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94de71bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode_image(x[0].to('cuda')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6963581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75b9d097",
   "metadata": {},
   "source": [
    "# Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba459549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('                                 FINAL REPORT\\n EXAMINATION:  CHEST (PA AND LAT)\\n \\n INDICATION:  ___F with new onset ascites  // eval for infection\\n \\n TECHNIQUE:  Chest PA and lateral\\n \\n COMPARISON:  None.\\n \\n FINDINGS: \\n \\n There is no focal consolidation, pleural effusion or pneumothorax.  Bilateral\\n nodular opacities that most likely represent nipple shadows. The\\n cardiomediastinal silhouette is normal.  Clips project over the left lung,\\n potentially within the breast. The imaged upper abdomen is unremarkable.\\n Chronic deformity of the posterior left sixth and seventh ribs are noted.\\n \\n IMPRESSION: \\n \\n No acute cardiopulmonary process.\\n',\n",
       " '                                 FINAL REPORT\\n EXAMINATION:  CHEST (PA AND LAT)\\n \\n INDICATION:  History: ___F with shortness of breath\\n \\n TECHNIQUE:  Chest PA and lateral\\n \\n COMPARISON:  ___\\n \\n FINDINGS: \\n \\n The cardiac, mediastinal and hilar contours are normal. Pulmonary vasculature\\n is normal.  Lungs are clear. No pleural effusion or pneumothorax is present.\\n Multiple clips are again seen projecting over the left breast.  Remote\\n left-sided rib fractures are also re- demonstrated.\\n \\n IMPRESSION: \\n \\n No acute cardiopulmonary abnormality.\\n',\n",
       " '                                 FINAL REPORT\\n STUDY:  PA and lateral chest x-ray.\\n \\n COMPARISON EXAM:  PA and lateral chest x-ray, ___.\\n \\n INDICATION:  ___-year-old woman with left supraclavicular fullness for several\\n months with history of right lung carcinoid tumor.\\n \\n FINDINGS:  Heart size is normal.  Mediastinal contours are normal with mild\\n aortic tortuosity.  Post-surgical changes in the right hemithorax are stable\\n including thickening of the pleura along the costal surface and blunting of\\n the costophrenic sulcus.  The right sixth rib surgical fracture is\\n redemonstrated.  There are no new lung nodules identified. \\n \\n IMPRESSION:  Stable chest radiograph.\\n',\n",
       " '                                 FINAL REPORT\\n REASON FOR EXAMINATION:  Evaluation of the patient with history of carcinoid\\n with intermittent dyspnea on exertion.\\n \\n PA and lateral upright chest radiographs were reviewed in comparison to ___.\\n \\n Heart size is normal.  Mediastinum is normal.  The post-surgical changes in\\n the right hemithorax are stable including thickening of the pleura along the\\n costal surface and blunting of the costophrenic sulcus.  The surgical fracture\\n of the right sixth rib is redemonstrated.  No new abnormalities are\\n demonstrated within the limitations of the chest radiograph technique.  Lung\\n volumes are preserved.\\n',\n",
       " '                                 FINAL REPORT\\n EXAMINATION:  CHEST (PA AND LAT)\\n \\n INDICATION:  ___ year old woman with productive cough and bilateral rales on\\n exam.  // r/o CHF or pneumonia      PRODUCTIVE COUGH R/O CHF OR PNEUMONIA\\n \\n IMPRESSION: \\n \\n Compared to chest radiographs since ___, most recently one ___.\\n \\n Previous mild pulmonary edema and possible concurrent pneumonia has all\\n cleared.  Heart is top-normal size, improved, and pleural effusions have\\n resolved.  Right hilar vessels are still enlarged, perhaps due to pulmonary\\n arterial hypertension.  Lateral view shows atherosclerotic coronary\\n calcification in the left circumflex.\\n',\n",
       " '                                 FINAL REPORT\\n EXAMINATION:  CHEST (PA AND LAT)\\n \\n INDICATION:  History: ___F with dyspnea\\n \\n TECHNIQUE:  Chest PA and lateral\\n \\n COMPARISON:  ___\\n \\n FINDINGS: \\n \\n Heart size remains mild to moderately enlarged.  The aorta is tortuous and\\n diffusely calcified.  Mediastinal and hilar contours are otherwise unchanged. \\n Previous pattern of mild pulmonary edema has essentially resolved.  Mild\\n atelectasis is seen in the lung bases without focal consolidation.  Blunting\\n of the costophrenic angles bilaterally suggests trace bilateral pleural\\n effusions, not substantially changed in the interval.  No pneumothorax is\\n present.\\n \\n IMPRESSION: \\n \\n Interval resolution of previously seen mild pulmonary edema with trace\\n bilateral pleural effusions.\\n',\n",
       " '                                 FINAL REPORT\\n EXAMINATION:  CHEST (PA AND LAT)\\n \\n INDICATION:  ___ year old woman with CKD with increased dyspnea and cough x 2\\n weeks.  // R/o pulmonary edema or infiltrate      R/o pulmonary edema or\\n infiltrate\\n \\n IMPRESSION: \\n \\n As compared to ___, the lung volumes have slightly decreased.  Signs\\n of mild overinflation and moderate pleural effusions persist.  Moderate\\n cardiomegaly.  Elongation of the descending aorta.  No pneumonia.\\n',\n",
       " '                                 FINAL REPORT\\n INDICATION:  Shortness of breath.\\n \\n COMPARISONS:  ___, ___.\\n \\n FINDINGS: PA and lateral views of the chest demonstrate low lung volumes. \\n Tiny bilateral pleural effusions are new since ___.  No signs of\\n pneumonia or pulmonary vascular congestion.  Heart is top normal in size\\n though this is stable. Aorta is markedly tortuous, unchanged.  Aortic arch\\n calcifications are seen.  There is no pneumothorax.  No focal consolidation. \\n Partially imaged upper abdomen is unremarkable.\\n \\n IMPRESSION: Tiny pleural effusions, new. Otherwise unremarkable.\\n')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3690548b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data2Seq()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_tokenizer_text = Data2Seq(modality='text',dim=768)\n",
    "\n",
    "auto_tokenizer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a417e0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (127 > 77). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([127])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(auto_tokenizer_text(x[1][0])['input_ids']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3f79ef6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406.,  1755.,  2417., 20970.,   281., 10563.,   263.,  2217.,   537.,\n",
       "         28437.,   264., 38539.,   281., 13530.,   325.,   593.,   686., 30527.,\n",
       "         22720.,  2454.,  3502.,  9703.,   556., 14774.,  1782.,   697.,  2319.,\n",
       "           281., 10563.,  2217.,   537., 26646., 14186.,   281.,  8906.,   269.,\n",
       "         16529.,   281.,   997.,   533.,   871., 30934., 41111.,   267.,   926.,\n",
       "         33948.,  1490.,  9364.,   541., 28714.,   617.,  4130.,  9203.,   269.,\n",
       "         25599.,   578.,   691.,  1652.,   676.,   546.,  1480.,   682.,  1096.,\n",
       "          5256.,  8406., 45987., 12971.,   269.,   518.,  6211.,  3693.,   570.,\n",
       "          1761., 19178., 26149.,   533.,  5967.,   269., 16594.,  1965.,   962.,\n",
       "           518.,  1823., 16271.,   267., 16508.,  4154.,   518.,  9475.,   269.,\n",
       "           518.,  2316.,   538.,  7067.,   596.,  2164.,   576.,   533.,   569.,\n",
       "         12404.,   269., 13677.,   561., 42290.,   539.,   518., 22881.,  2498.,\n",
       "          1823., 12909.,   537., 17568., 17519.,   631., 22501.,   269., 14468.,\n",
       "           281.,   871., 19734.,  6211., 39858.,   749.,   856.,  4078.,   269.,\n",
       "         49407.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_padding(torch.tensor(auto_tokenizer_text.embed(x[1])['input_ids'][0]).unsqueeze(dim=0), 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba1f1615",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_padding(torch.tensor(auto_tokenizer_text.embed(x[1])['input_ids'][0]).unsqueeze(dim=0), 768).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c08b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c966bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c7deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f017f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embeddings(text, tar_dim=768):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load('ViT-B/32', device)\n",
    "    text_tensor = clip.tokenize(text, truncate=True)\n",
    "    encoding = model.encode_text(text_tensor.to('cuda'))\n",
    "    encoding = zero_padding(encoding, tar_dim, device)\n",
    "    encoding = encoding.unsqueeze(dim=1)\n",
    "    return encoding.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cee8b97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0035, -0.0186,  0.0605,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.1467,  0.0156,  0.0248,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.1428,  0.1348, -0.0100,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0020,  0.0623,  0.0526,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0994, -0.1002,  0.1600,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0751,  0.1327, -0.2786,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       device='cuda:0', grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_embeddings(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63880071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_embeddings(x[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee42695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5655d67",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef1703d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (1): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (2): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (3): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (4): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (5): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (6): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (7): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (8): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (9): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (10): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (11): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efeb3941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text\n",
    "encoder(get_text_embeddings(x[1])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46d16d90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# image\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m encoder(\u001b[43mauto_tokenizer_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/meta_test/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mData2Seq.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,data):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m ]:\n\u001b[0;32m---> 14\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality \u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     16\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/meta_test/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m H \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m W \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m1\u001b[39m], \\\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match model (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# flatten: [B, C, H, W] -> [B, C, HW]\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# transpose: [B, C, HW] -> [B, HW, C]\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/meta_test/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/meta_test/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/meta_test/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "# image\n",
    "\n",
    "encoder(auto_tokenizer_img(x[0].to('cuda'))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00552b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (1): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (2): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (3): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (4): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (5): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (6): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (7): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (8): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (9): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (10): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       "  (11): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls1): Identity()\n",
       "    (drop_path1): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ls2): Identity()\n",
       "    (drop_path2): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e696adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=14, bias=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_head = nn.Linear(768, 14)\n",
    "classification_head.to('cuda')\n",
    "classification_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "106004cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 14])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_head(encoder(get_text_embeddings(x[1]))).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef1f0164",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m classification_head(encoder(\u001b[43mauto_tokenizer_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/meta_test/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mData2Seq.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,data):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m ]:\n\u001b[0;32m---> 14\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality \u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     16\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/meta_test/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m H \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m W \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m1\u001b[39m], \\\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match model (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# flatten: [B, C, H, W] -> [B, C, HW]\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# transpose: [B, C, HW] -> [B, HW, C]\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/meta_test/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/meta_test/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/meta_test/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "classification_head(encoder(auto_tokenizer_img(x[0].to('cuda')))).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67870639",
   "metadata": {},
   "source": [
    "# Meta Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c28098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meta_Transformer(nn.Module):\n",
    "    def __init__(self, num_classes, checkpoint_path):\n",
    "        super().__init__()\n",
    "        self.image_embedding = PatchEmbed().to('cuda')\n",
    "        \n",
    "        self.meta_encoder = load_meta_transformer(checkpoint_path).to('cuda')\n",
    "        for param in self.meta_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool1d(1).to('cuda')\n",
    "        self.classification_head = nn.Linear(768, num_classes).to('cuda')\n",
    "        self.text_encoder, _ = clip.load('ViT-B/32')\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, list):\n",
    "            image, text = x[0], x[1]\n",
    "        else:\n",
    "            image = x\n",
    "            text = None\n",
    "\n",
    "        # Process the image\n",
    "        image_embedding = self.image_embedding(image.to('cuda'))\n",
    "        image_encoding = self.meta_encoder(image_embedding)\n",
    "        image_encoding = self.global_avg_pooling(image_encoding.permute(0, 2, 1)).squeeze(dim=2)\n",
    "        y_hat_img = self.classification_head(image_encoding)\n",
    "\n",
    "        if text is not None:\n",
    "            # Process the text\n",
    "            text_embedding = get_text_embeddings(text=text, tar_dim=768, model=self.text_encoder)\n",
    "            text_encoding = self.meta_encoder(text_embedding)\n",
    "            y_hat_text = self.classification_head(text_encoding)\n",
    "            return y_hat_img, y_hat_text.squeeze()\n",
    "\n",
    "        return y_hat_img\n",
    "    \n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "\n",
    "        # flatten: [B, C, H, W] -> [B, C, HW]\n",
    "        # transpose: [B, C, HW] -> [B, HW, C]\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x    \n",
    "    \n",
    "def load_meta_transformer(checkpoint_path):\n",
    "    ckpt = torch.load(checkpoint_path)\n",
    "    encoder = nn.Sequential(*[\n",
    "                Block(\n",
    "                    dim=768,\n",
    "                    num_heads=12,\n",
    "                    mlp_ratio=4.,\n",
    "                    qkv_bias=True,\n",
    "                    norm_layer=nn.LayerNorm,\n",
    "                    act_layer=nn.GELU\n",
    "                )\n",
    "                for i in range(12)])\n",
    "    encoder.load_state_dict(ckpt,strict=True)\n",
    "    print('Meta Transformer initilaized with pretrained weights.')\n",
    "    return encoder\n",
    "\n",
    "def get_text_embeddings(text, tar_dim=768, model=None):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    text_tensor = clip.tokenize(text, truncate=True)\n",
    "    encoding = model.encode_text(text_tensor.to('cuda'))\n",
    "    encoding = zero_padding(encoding, tar_dim, device)\n",
    "    encoding = encoding.unsqueeze(dim=1)\n",
    "    return encoding.to('cuda').detach()\n",
    "\n",
    "def zero_padding(text_tensor, tar_dim, device=None):\n",
    "    padding_size = tar_dim - text_tensor.shape[1]\n",
    "    zero_tensor = torch.zeros((text_tensor.shape[0], padding_size), device=device)\n",
    "    padded_tensor = torch.cat([text_tensor, zero_tensor], dim=1)\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61a7ce57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[-1.8597, -1.8295, -1.8144,  ..., -1.8597, -1.8597, -1.8597],\n",
      "          [-1.7692, -1.7541, -1.7390,  ..., -1.7390, -1.7541, -1.7541],\n",
      "          [-1.6485, -1.6636, -1.6938,  ..., -1.6938, -1.7239, -1.7088],\n",
      "          ...,\n",
      "          [-0.0045,  0.2670,  0.2821,  ..., -1.7692, -1.7541, -1.7541],\n",
      "          [ 0.0257,  0.3274,  0.3274,  ..., -1.7692, -1.7541, -1.7541],\n",
      "          [ 0.0257,  0.3575,  0.3877,  ..., -1.7692, -1.7692, -1.7541]],\n",
      "\n",
      "         [[-1.8597, -1.8295, -1.8144,  ..., -1.8597, -1.8597, -1.8597],\n",
      "          [-1.7692, -1.7541, -1.7390,  ..., -1.7390, -1.7541, -1.7541],\n",
      "          [-1.6485, -1.6636, -1.6938,  ..., -1.6938, -1.7239, -1.7088],\n",
      "          ...,\n",
      "          [-0.0045,  0.2670,  0.2821,  ..., -1.7692, -1.7541, -1.7541],\n",
      "          [ 0.0257,  0.3274,  0.3274,  ..., -1.7692, -1.7541, -1.7541],\n",
      "          [ 0.0257,  0.3575,  0.3877,  ..., -1.7692, -1.7692, -1.7541]],\n",
      "\n",
      "         [[-1.8597, -1.8295, -1.8144,  ..., -1.8597, -1.8597, -1.8597],\n",
      "          [-1.7692, -1.7541, -1.7390,  ..., -1.7390, -1.7541, -1.7541],\n",
      "          [-1.6485, -1.6636, -1.6938,  ..., -1.6938, -1.7239, -1.7088],\n",
      "          ...,\n",
      "          [-0.0045,  0.2670,  0.2821,  ..., -1.7692, -1.7541, -1.7541],\n",
      "          [ 0.0257,  0.3274,  0.3274,  ..., -1.7692, -1.7541, -1.7541],\n",
      "          [ 0.0257,  0.3575,  0.3877,  ..., -1.7692, -1.7692, -1.7541]]],\n",
      "\n",
      "\n",
      "        [[[-1.6485, -1.6485, -1.6485,  ...,  0.8854,  0.8703,  0.8854],\n",
      "          [-1.7088, -1.7088, -1.7088,  ...,  0.8854,  0.8703,  0.8703],\n",
      "          [-1.7541, -1.7390, -1.7390,  ...,  0.6894,  0.7044,  0.7044],\n",
      "          ...,\n",
      "          [ 1.5642,  1.5642,  1.5642,  ...,  1.4586,  1.4586,  1.4586],\n",
      "          [ 1.5792,  1.5943,  1.5943,  ...,  1.4737,  1.4737,  1.4737],\n",
      "          [ 1.5943,  1.5943,  1.5943,  ...,  1.4887,  1.4887,  1.4737]],\n",
      "\n",
      "         [[-1.6485, -1.6485, -1.6485,  ...,  0.8854,  0.8703,  0.8854],\n",
      "          [-1.7088, -1.7088, -1.7088,  ...,  0.8854,  0.8703,  0.8703],\n",
      "          [-1.7541, -1.7390, -1.7390,  ...,  0.6894,  0.7044,  0.7044],\n",
      "          ...,\n",
      "          [ 1.5642,  1.5642,  1.5642,  ...,  1.4586,  1.4586,  1.4586],\n",
      "          [ 1.5792,  1.5943,  1.5943,  ...,  1.4737,  1.4737,  1.4737],\n",
      "          [ 1.5943,  1.5943,  1.5943,  ...,  1.4887,  1.4887,  1.4737]],\n",
      "\n",
      "         [[-1.6485, -1.6485, -1.6485,  ...,  0.8854,  0.8703,  0.8854],\n",
      "          [-1.7088, -1.7088, -1.7088,  ...,  0.8854,  0.8703,  0.8703],\n",
      "          [-1.7541, -1.7390, -1.7390,  ...,  0.6894,  0.7044,  0.7044],\n",
      "          ...,\n",
      "          [ 1.5642,  1.5642,  1.5642,  ...,  1.4586,  1.4586,  1.4586],\n",
      "          [ 1.5792,  1.5943,  1.5943,  ...,  1.4737,  1.4737,  1.4737],\n",
      "          [ 1.5943,  1.5943,  1.5943,  ...,  1.4887,  1.4887,  1.4737]]],\n",
      "\n",
      "\n",
      "        [[[-1.8295, -1.8295, -1.8295,  ..., -1.7541, -1.7692, -1.7843],\n",
      "          [-1.8295, -1.8295, -1.8295,  ..., -1.7239, -1.7390, -1.7692],\n",
      "          [-1.8144, -1.8144, -1.8144,  ..., -1.6787, -1.7088, -1.7390],\n",
      "          ...,\n",
      "          [ 1.7602,  1.7452,  1.7753,  ...,  1.4435,  1.3832,  1.3681],\n",
      "          [ 1.7602,  1.7602,  1.7753,  ...,  1.3983,  1.3681,  1.3832],\n",
      "          [ 1.7602,  1.7602,  1.7904,  ...,  1.4284,  1.4133,  1.4284]],\n",
      "\n",
      "         [[-1.8295, -1.8295, -1.8295,  ..., -1.7541, -1.7692, -1.7843],\n",
      "          [-1.8295, -1.8295, -1.8295,  ..., -1.7239, -1.7390, -1.7692],\n",
      "          [-1.8144, -1.8144, -1.8144,  ..., -1.6787, -1.7088, -1.7390],\n",
      "          ...,\n",
      "          [ 1.7602,  1.7452,  1.7753,  ...,  1.4435,  1.3832,  1.3681],\n",
      "          [ 1.7602,  1.7602,  1.7753,  ...,  1.3983,  1.3681,  1.3832],\n",
      "          [ 1.7602,  1.7602,  1.7904,  ...,  1.4284,  1.4133,  1.4284]],\n",
      "\n",
      "         [[-1.8295, -1.8295, -1.8295,  ..., -1.7541, -1.7692, -1.7843],\n",
      "          [-1.8295, -1.8295, -1.8295,  ..., -1.7239, -1.7390, -1.7692],\n",
      "          [-1.8144, -1.8144, -1.8144,  ..., -1.6787, -1.7088, -1.7390],\n",
      "          ...,\n",
      "          [ 1.7602,  1.7452,  1.7753,  ...,  1.4435,  1.3832,  1.3681],\n",
      "          [ 1.7602,  1.7602,  1.7753,  ...,  1.3983,  1.3681,  1.3832],\n",
      "          [ 1.7602,  1.7602,  1.7904,  ...,  1.4284,  1.4133,  1.4284]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.7239, -1.7541, -1.7843,  ..., -1.7088, -1.5429, -1.6485],\n",
      "          [-1.7239, -1.7541, -1.7843,  ..., -1.3016, -0.7134, -0.7737],\n",
      "          [-1.7239, -1.7541, -1.7692,  ..., -1.5882, -0.9849, -1.5128],\n",
      "          ...,\n",
      "          [-0.3966, -0.2910, -0.2156,  ...,  1.6396,  1.5792,  1.4737],\n",
      "          [-0.3665, -0.2458, -0.1553,  ...,  1.5792,  1.5340,  1.4737],\n",
      "          [-0.3363, -0.2307, -0.1402,  ...,  1.5642,  1.5491,  1.5038]],\n",
      "\n",
      "         [[-1.7239, -1.7541, -1.7843,  ..., -1.7088, -1.5429, -1.6485],\n",
      "          [-1.7239, -1.7541, -1.7843,  ..., -1.3016, -0.7134, -0.7737],\n",
      "          [-1.7239, -1.7541, -1.7692,  ..., -1.5882, -0.9849, -1.5128],\n",
      "          ...,\n",
      "          [-0.3966, -0.2910, -0.2156,  ...,  1.6396,  1.5792,  1.4737],\n",
      "          [-0.3665, -0.2458, -0.1553,  ...,  1.5792,  1.5340,  1.4737],\n",
      "          [-0.3363, -0.2307, -0.1402,  ...,  1.5642,  1.5491,  1.5038]],\n",
      "\n",
      "         [[-1.7239, -1.7541, -1.7843,  ..., -1.7088, -1.5429, -1.6485],\n",
      "          [-1.7239, -1.7541, -1.7843,  ..., -1.3016, -0.7134, -0.7737],\n",
      "          [-1.7239, -1.7541, -1.7692,  ..., -1.5882, -0.9849, -1.5128],\n",
      "          ...,\n",
      "          [-0.3966, -0.2910, -0.2156,  ...,  1.6396,  1.5792,  1.4737],\n",
      "          [-0.3665, -0.2458, -0.1553,  ...,  1.5792,  1.5340,  1.4737],\n",
      "          [-0.3363, -0.2307, -0.1402,  ...,  1.5642,  1.5491,  1.5038]]],\n",
      "\n",
      "\n",
      "        [[[-1.7692, -1.7692, -1.7692,  ..., -1.7390, -1.7541, -1.7541],\n",
      "          [-1.7692, -1.7692, -1.7692,  ..., -1.7390, -1.7541, -1.7541],\n",
      "          [-1.7692, -1.7692, -1.7692,  ..., -1.7390, -1.7541, -1.7541],\n",
      "          ...,\n",
      "          [-1.2111, -1.1206, -1.0150,  ...,  0.7497,  0.6592,  0.5989],\n",
      "          [-1.2262, -1.1206, -1.0150,  ...,  0.7648,  0.6592,  0.6139],\n",
      "          [-1.2262, -1.1206, -1.0150,  ...,  0.7648,  0.6894,  0.6441]],\n",
      "\n",
      "         [[-1.7692, -1.7692, -1.7692,  ..., -1.7390, -1.7541, -1.7541],\n",
      "          [-1.7692, -1.7692, -1.7692,  ..., -1.7390, -1.7541, -1.7541],\n",
      "          [-1.7692, -1.7692, -1.7692,  ..., -1.7390, -1.7541, -1.7541],\n",
      "          ...,\n",
      "          [-1.2111, -1.1206, -1.0150,  ...,  0.7497,  0.6592,  0.5989],\n",
      "          [-1.2262, -1.1206, -1.0150,  ...,  0.7648,  0.6592,  0.6139],\n",
      "          [-1.2262, -1.1206, -1.0150,  ...,  0.7648,  0.6894,  0.6441]],\n",
      "\n",
      "         [[-1.7692, -1.7692, -1.7692,  ..., -1.7390, -1.7541, -1.7541],\n",
      "          [-1.7692, -1.7692, -1.7692,  ..., -1.7390, -1.7541, -1.7541],\n",
      "          [-1.7692, -1.7692, -1.7692,  ..., -1.7390, -1.7541, -1.7541],\n",
      "          ...,\n",
      "          [-1.2111, -1.1206, -1.0150,  ...,  0.7497,  0.6592,  0.5989],\n",
      "          [-1.2262, -1.1206, -1.0150,  ...,  0.7648,  0.6592,  0.6139],\n",
      "          [-1.2262, -1.1206, -1.0150,  ...,  0.7648,  0.6894,  0.6441]]],\n",
      "\n",
      "\n",
      "        [[[-1.7239, -1.7239, -1.7239,  ..., -0.6681, -1.4223, -1.8295],\n",
      "          [-1.7239, -1.7239, -1.7239,  ..., -0.7284, -1.2413, -1.8597],\n",
      "          [-1.7239, -1.7239, -1.7239,  ..., -1.0754, -0.6983, -1.6485],\n",
      "          ...,\n",
      "          [ 0.4028,  0.4480,  0.4933,  ...,  0.6592,  0.6290,  0.5838],\n",
      "          [ 0.3877,  0.4480,  0.4933,  ...,  0.6743,  0.6441,  0.6139],\n",
      "          [ 0.3877,  0.4329,  0.4631,  ...,  0.6743,  0.6441,  0.6290]],\n",
      "\n",
      "         [[-1.7239, -1.7239, -1.7239,  ..., -0.6681, -1.4223, -1.8295],\n",
      "          [-1.7239, -1.7239, -1.7239,  ..., -0.7284, -1.2413, -1.8597],\n",
      "          [-1.7239, -1.7239, -1.7239,  ..., -1.0754, -0.6983, -1.6485],\n",
      "          ...,\n",
      "          [ 0.4028,  0.4480,  0.4933,  ...,  0.6592,  0.6290,  0.5838],\n",
      "          [ 0.3877,  0.4480,  0.4933,  ...,  0.6743,  0.6441,  0.6139],\n",
      "          [ 0.3877,  0.4329,  0.4631,  ...,  0.6743,  0.6441,  0.6290]],\n",
      "\n",
      "         [[-1.7239, -1.7239, -1.7239,  ..., -0.6681, -1.4223, -1.8295],\n",
      "          [-1.7239, -1.7239, -1.7239,  ..., -0.7284, -1.2413, -1.8597],\n",
      "          [-1.7239, -1.7239, -1.7239,  ..., -1.0754, -0.6983, -1.6485],\n",
      "          ...,\n",
      "          [ 0.4028,  0.4480,  0.4933,  ...,  0.6592,  0.6290,  0.5838],\n",
      "          [ 0.3877,  0.4480,  0.4933,  ...,  0.6743,  0.6441,  0.6139],\n",
      "          [ 0.3877,  0.4329,  0.4631,  ...,  0.6743,  0.6441,  0.6290]]]]), ('                                 FINAL REPORT\\n EXAMINATION:  CHEST (PA AND LAT)\\n \\n INDICATION:  ___F with new onset ascites  // eval for infection\\n \\n TECHNIQUE:  Chest PA and lateral\\n \\n COMPARISON:  None.\\n \\n FINDINGS: \\n \\n There is no focal consolidation, pleural effusion or pneumothorax.  Bilateral\\n nodular opacities that most likely represent nipple shadows. The\\n cardiomediastinal silhouette is normal.  Clips project over the left lung,\\n potentially within the breast. The imaged upper abdomen is unremarkable.\\n Chronic deformity of the posterior left sixth and seventh ribs are noted.\\n \\n IMPRESSION: \\n \\n No acute cardiopulmonary process.\\n', '                                 FINAL REPORT\\n EXAMINATION:  CHEST (PA AND LAT)\\n \\n INDICATION:  History: ___F with shortness of breath\\n \\n TECHNIQUE:  Chest PA and lateral\\n \\n COMPARISON:  ___\\n \\n FINDINGS: \\n \\n The cardiac, mediastinal and hilar contours are normal. Pulmonary vasculature\\n is normal.  Lungs are clear. No pleural effusion or pneumothorax is present.\\n Multiple clips are again seen projecting over the left breast.  Remote\\n left-sided rib fractures are also re- demonstrated.\\n \\n IMPRESSION: \\n \\n No acute cardiopulmonary abnormality.\\n', '                                 FINAL REPORT\\n STUDY:  PA and lateral chest x-ray.\\n \\n COMPARISON EXAM:  PA and lateral chest x-ray, ___.\\n \\n INDICATION:  ___-year-old woman with left supraclavicular fullness for several\\n months with history of right lung carcinoid tumor.\\n \\n FINDINGS:  Heart size is normal.  Mediastinal contours are normal with mild\\n aortic tortuosity.  Post-surgical changes in the right hemithorax are stable\\n including thickening of the pleura along the costal surface and blunting of\\n the costophrenic sulcus.  The right sixth rib surgical fracture is\\n redemonstrated.  There are no new lung nodules identified. \\n \\n IMPRESSION:  Stable chest radiograph.\\n', '                                 FINAL REPORT\\n REASON FOR EXAMINATION:  Evaluation of the patient with history of carcinoid\\n with intermittent dyspnea on exertion.\\n \\n PA and lateral upright chest radiographs were reviewed in comparison to ___.\\n \\n Heart size is normal.  Mediastinum is normal.  The post-surgical changes in\\n the right hemithorax are stable including thickening of the pleura along the\\n costal surface and blunting of the costophrenic sulcus.  The surgical fracture\\n of the right sixth rib is redemonstrated.  No new abnormalities are\\n demonstrated within the limitations of the chest radiograph technique.  Lung\\n volumes are preserved.\\n', '                                 FINAL REPORT\\n EXAMINATION:  CHEST (PA AND LAT)\\n \\n INDICATION:  ___ year old woman with productive cough and bilateral rales on\\n exam.  // r/o CHF or pneumonia      PRODUCTIVE COUGH R/O CHF OR PNEUMONIA\\n \\n IMPRESSION: \\n \\n Compared to chest radiographs since ___, most recently one ___.\\n \\n Previous mild pulmonary edema and possible concurrent pneumonia has all\\n cleared.  Heart is top-normal size, improved, and pleural effusions have\\n resolved.  Right hilar vessels are still enlarged, perhaps due to pulmonary\\n arterial hypertension.  Lateral view shows atherosclerotic coronary\\n calcification in the left circumflex.\\n', '                                 FINAL REPORT\\n EXAMINATION:  CHEST (PA AND LAT)\\n \\n INDICATION:  History: ___F with dyspnea\\n \\n TECHNIQUE:  Chest PA and lateral\\n \\n COMPARISON:  ___\\n \\n FINDINGS: \\n \\n Heart size remains mild to moderately enlarged.  The aorta is tortuous and\\n diffusely calcified.  Mediastinal and hilar contours are otherwise unchanged. \\n Previous pattern of mild pulmonary edema has essentially resolved.  Mild\\n atelectasis is seen in the lung bases without focal consolidation.  Blunting\\n of the costophrenic angles bilaterally suggests trace bilateral pleural\\n effusions, not substantially changed in the interval.  No pneumothorax is\\n present.\\n \\n IMPRESSION: \\n \\n Interval resolution of previously seen mild pulmonary edema with trace\\n bilateral pleural effusions.\\n', '                                 FINAL REPORT\\n EXAMINATION:  CHEST (PA AND LAT)\\n \\n INDICATION:  ___ year old woman with CKD with increased dyspnea and cough x 2\\n weeks.  // R/o pulmonary edema or infiltrate      R/o pulmonary edema or\\n infiltrate\\n \\n IMPRESSION: \\n \\n As compared to ___, the lung volumes have slightly decreased.  Signs\\n of mild overinflation and moderate pleural effusions persist.  Moderate\\n cardiomegaly.  Elongation of the descending aorta.  No pneumonia.\\n', '                                 FINAL REPORT\\n INDICATION:  Shortness of breath.\\n \\n COMPARISONS:  ___, ___.\\n \\n FINDINGS: PA and lateral views of the chest demonstrate low lung volumes. \\n Tiny bilateral pleural effusions are new since ___.  No signs of\\n pneumonia or pulmonary vascular congestion.  Heart is top normal in size\\n though this is stable. Aorta is markedly tortuous, unchanged.  Aortic arch\\n calcifications are seen.  There is no pneumothorax.  No focal consolidation. \\n Partially imaged upper abdomen is unremarkable.\\n \\n IMPRESSION: Tiny pleural effusions, new. Otherwise unremarkable.\\n')]\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fff14458",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta Transformer initilaized with pretrained weights.\n"
     ]
    }
   ],
   "source": [
    "meta_transformer = Meta_Transformer(14, '/home/fe/baur/Downloads/Meta-Transformer_base_patch16_encoder (1).pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "481f801d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[-1.8597, -1.8295, -1.8144,  ..., -1.8597, -1.8597, -1.8597],\n",
       "           [-1.7692, -1.7541, -1.7390,  ..., -1.7390, -1.7541, -1.7541],\n",
       "           [-1.6485, -1.6636, -1.6938,  ..., -1.6938, -1.7239, -1.7088],\n",
       "           ...,\n",
       "           [-0.0045,  0.2670,  0.2821,  ..., -1.7692, -1.7541, -1.7541],\n",
       "           [ 0.0257,  0.3274,  0.3274,  ..., -1.7692, -1.7541, -1.7541],\n",
       "           [ 0.0257,  0.3575,  0.3877,  ..., -1.7692, -1.7692, -1.7541]],\n",
       " \n",
       "          [[-1.8597, -1.8295, -1.8144,  ..., -1.8597, -1.8597, -1.8597],\n",
       "           [-1.7692, -1.7541, -1.7390,  ..., -1.7390, -1.7541, -1.7541],\n",
       "           [-1.6485, -1.6636, -1.6938,  ..., -1.6938, -1.7239, -1.7088],\n",
       "           ...,\n",
       "           [-0.0045,  0.2670,  0.2821,  ..., -1.7692, -1.7541, -1.7541],\n",
       "           [ 0.0257,  0.3274,  0.3274,  ..., -1.7692, -1.7541, -1.7541],\n",
       "           [ 0.0257,  0.3575,  0.3877,  ..., -1.7692, -1.7692, -1.7541]],\n",
       " \n",
       "          [[-1.8597, -1.8295, -1.8144,  ..., -1.8597, -1.8597, -1.8597],\n",
       "           [-1.7692, -1.7541, -1.7390,  ..., -1.7390, -1.7541, -1.7541],\n",
       "           [-1.6485, -1.6636, -1.6938,  ..., -1.6938, -1.7239, -1.7088],\n",
       "           ...,\n",
       "           [-0.0045,  0.2670,  0.2821,  ..., -1.7692, -1.7541, -1.7541],\n",
       "           [ 0.0257,  0.3274,  0.3274,  ..., -1.7692, -1.7541, -1.7541],\n",
       "           [ 0.0257,  0.3575,  0.3877,  ..., -1.7692, -1.7692, -1.7541]]],\n",
       " \n",
       " \n",
       "         [[[-1.6485, -1.6485, -1.6485,  ...,  0.8854,  0.8703,  0.8854],\n",
       "           [-1.7088, -1.7088, -1.7088,  ...,  0.8854,  0.8703,  0.8703],\n",
       "           [-1.7541, -1.7390, -1.7390,  ...,  0.6894,  0.7044,  0.7044],\n",
       "           ...,\n",
       "           [ 1.5642,  1.5642,  1.5642,  ...,  1.4586,  1.4586,  1.4586],\n",
       "           [ 1.5792,  1.5943,  1.5943,  ...,  1.4737,  1.4737,  1.4737],\n",
       "           [ 1.5943,  1.5943,  1.5943,  ...,  1.4887,  1.4887,  1.4737]],\n",
       " \n",
       "          [[-1.6485, -1.6485, -1.6485,  ...,  0.8854,  0.8703,  0.8854],\n",
       "           [-1.7088, -1.7088, -1.7088,  ...,  0.8854,  0.8703,  0.8703],\n",
       "           [-1.7541, -1.7390, -1.7390,  ...,  0.6894,  0.7044,  0.7044],\n",
       "           ...,\n",
       "           [ 1.5642,  1.5642,  1.5642,  ...,  1.4586,  1.4586,  1.4586],\n",
       "           [ 1.5792,  1.5943,  1.5943,  ...,  1.4737,  1.4737,  1.4737],\n",
       "           [ 1.5943,  1.5943,  1.5943,  ...,  1.4887,  1.4887,  1.4737]],\n",
       " \n",
       "          [[-1.6485, -1.6485, -1.6485,  ...,  0.8854,  0.8703,  0.8854],\n",
       "           [-1.7088, -1.7088, -1.7088,  ...,  0.8854,  0.8703,  0.8703],\n",
       "           [-1.7541, -1.7390, -1.7390,  ...,  0.6894,  0.7044,  0.7044],\n",
       "           ...,\n",
       "           [ 1.5642,  1.5642,  1.5642,  ...,  1.4586,  1.4586,  1.4586],\n",
       "           [ 1.5792,  1.5943,  1.5943,  ...,  1.4737,  1.4737,  1.4737],\n",
       "           [ 1.5943,  1.5943,  1.5943,  ...,  1.4887,  1.4887,  1.4737]]],\n",
       " \n",
       " \n",
       "         [[[-1.8295, -1.8295, -1.8295,  ..., -1.7541, -1.7692, -1.7843],\n",
       "           [-1.8295, -1.8295, -1.8295,  ..., -1.7239, -1.7390, -1.7692],\n",
       "           [-1.8144, -1.8144, -1.8144,  ..., -1.6787, -1.7088, -1.7390],\n",
       "           ...,\n",
       "           [ 1.7602,  1.7452,  1.7753,  ...,  1.4435,  1.3832,  1.3681],\n",
       "           [ 1.7602,  1.7602,  1.7753,  ...,  1.3983,  1.3681,  1.3832],\n",
       "           [ 1.7602,  1.7602,  1.7904,  ...,  1.4284,  1.4133,  1.4284]],\n",
       " \n",
       "          [[-1.8295, -1.8295, -1.8295,  ..., -1.7541, -1.7692, -1.7843],\n",
       "           [-1.8295, -1.8295, -1.8295,  ..., -1.7239, -1.7390, -1.7692],\n",
       "           [-1.8144, -1.8144, -1.8144,  ..., -1.6787, -1.7088, -1.7390],\n",
       "           ...,\n",
       "           [ 1.7602,  1.7452,  1.7753,  ...,  1.4435,  1.3832,  1.3681],\n",
       "           [ 1.7602,  1.7602,  1.7753,  ...,  1.3983,  1.3681,  1.3832],\n",
       "           [ 1.7602,  1.7602,  1.7904,  ...,  1.4284,  1.4133,  1.4284]],\n",
       " \n",
       "          [[-1.8295, -1.8295, -1.8295,  ..., -1.7541, -1.7692, -1.7843],\n",
       "           [-1.8295, -1.8295, -1.8295,  ..., -1.7239, -1.7390, -1.7692],\n",
       "           [-1.8144, -1.8144, -1.8144,  ..., -1.6787, -1.7088, -1.7390],\n",
       "           ...,\n",
       "           [ 1.7602,  1.7452,  1.7753,  ...,  1.4435,  1.3832,  1.3681],\n",
       "           [ 1.7602,  1.7602,  1.7753,  ...,  1.3983,  1.3681,  1.3832],\n",
       "           [ 1.7602,  1.7602,  1.7904,  ...,  1.4284,  1.4133,  1.4284]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-1.7239, -1.7541, -1.7843,  ..., -1.7088, -1.5429, -1.6485],\n",
       "           [-1.7239, -1.7541, -1.7843,  ..., -1.3016, -0.7134, -0.7737],\n",
       "           [-1.7239, -1.7541, -1.7692,  ..., -1.5882, -0.9849, -1.5128],\n",
       "           ...,\n",
       "           [-0.3966, -0.2910, -0.2156,  ...,  1.6396,  1.5792,  1.4737],\n",
       "           [-0.3665, -0.2458, -0.1553,  ...,  1.5792,  1.5340,  1.4737],\n",
       "           [-0.3363, -0.2307, -0.1402,  ...,  1.5642,  1.5491,  1.5038]],\n",
       " \n",
       "          [[-1.7239, -1.7541, -1.7843,  ..., -1.7088, -1.5429, -1.6485],\n",
       "           [-1.7239, -1.7541, -1.7843,  ..., -1.3016, -0.7134, -0.7737],\n",
       "           [-1.7239, -1.7541, -1.7692,  ..., -1.5882, -0.9849, -1.5128],\n",
       "           ...,\n",
       "           [-0.3966, -0.2910, -0.2156,  ...,  1.6396,  1.5792,  1.4737],\n",
       "           [-0.3665, -0.2458, -0.1553,  ...,  1.5792,  1.5340,  1.4737],\n",
       "           [-0.3363, -0.2307, -0.1402,  ...,  1.5642,  1.5491,  1.5038]],\n",
       " \n",
       "          [[-1.7239, -1.7541, -1.7843,  ..., -1.7088, -1.5429, -1.6485],\n",
       "           [-1.7239, -1.7541, -1.7843,  ..., -1.3016, -0.7134, -0.7737],\n",
       "           [-1.7239, -1.7541, -1.7692,  ..., -1.5882, -0.9849, -1.5128],\n",
       "           ...,\n",
       "           [-0.3966, -0.2910, -0.2156,  ...,  1.6396,  1.5792,  1.4737],\n",
       "           [-0.3665, -0.2458, -0.1553,  ...,  1.5792,  1.5340,  1.4737],\n",
       "           [-0.3363, -0.2307, -0.1402,  ...,  1.5642,  1.5491,  1.5038]]],\n",
       " \n",
       " \n",
       "         [[[-1.7692, -1.7692, -1.7692,  ..., -1.7390, -1.7541, -1.7541],\n",
       "           [-1.7692, -1.7692, -1.7692,  ..., -1.7390, -1.7541, -1.7541],\n",
       "           [-1.7692, -1.7692, -1.7692,  ..., -1.7390, -1.7541, -1.7541],\n",
       "           ...,\n",
       "           [-1.2111, -1.1206, -1.0150,  ...,  0.7497,  0.6592,  0.5989],\n",
       "           [-1.2262, -1.1206, -1.0150,  ...,  0.7648,  0.6592,  0.6139],\n",
       "           [-1.2262, -1.1206, -1.0150,  ...,  0.7648,  0.6894,  0.6441]],\n",
       " \n",
       "          [[-1.7692, -1.7692, -1.7692,  ..., -1.7390, -1.7541, -1.7541],\n",
       "           [-1.7692, -1.7692, -1.7692,  ..., -1.7390, -1.7541, -1.7541],\n",
       "           [-1.7692, -1.7692, -1.7692,  ..., -1.7390, -1.7541, -1.7541],\n",
       "           ...,\n",
       "           [-1.2111, -1.1206, -1.0150,  ...,  0.7497,  0.6592,  0.5989],\n",
       "           [-1.2262, -1.1206, -1.0150,  ...,  0.7648,  0.6592,  0.6139],\n",
       "           [-1.2262, -1.1206, -1.0150,  ...,  0.7648,  0.6894,  0.6441]],\n",
       " \n",
       "          [[-1.7692, -1.7692, -1.7692,  ..., -1.7390, -1.7541, -1.7541],\n",
       "           [-1.7692, -1.7692, -1.7692,  ..., -1.7390, -1.7541, -1.7541],\n",
       "           [-1.7692, -1.7692, -1.7692,  ..., -1.7390, -1.7541, -1.7541],\n",
       "           ...,\n",
       "           [-1.2111, -1.1206, -1.0150,  ...,  0.7497,  0.6592,  0.5989],\n",
       "           [-1.2262, -1.1206, -1.0150,  ...,  0.7648,  0.6592,  0.6139],\n",
       "           [-1.2262, -1.1206, -1.0150,  ...,  0.7648,  0.6894,  0.6441]]],\n",
       " \n",
       " \n",
       "         [[[-1.7239, -1.7239, -1.7239,  ..., -0.6681, -1.4223, -1.8295],\n",
       "           [-1.7239, -1.7239, -1.7239,  ..., -0.7284, -1.2413, -1.8597],\n",
       "           [-1.7239, -1.7239, -1.7239,  ..., -1.0754, -0.6983, -1.6485],\n",
       "           ...,\n",
       "           [ 0.4028,  0.4480,  0.4933,  ...,  0.6592,  0.6290,  0.5838],\n",
       "           [ 0.3877,  0.4480,  0.4933,  ...,  0.6743,  0.6441,  0.6139],\n",
       "           [ 0.3877,  0.4329,  0.4631,  ...,  0.6743,  0.6441,  0.6290]],\n",
       " \n",
       "          [[-1.7239, -1.7239, -1.7239,  ..., -0.6681, -1.4223, -1.8295],\n",
       "           [-1.7239, -1.7239, -1.7239,  ..., -0.7284, -1.2413, -1.8597],\n",
       "           [-1.7239, -1.7239, -1.7239,  ..., -1.0754, -0.6983, -1.6485],\n",
       "           ...,\n",
       "           [ 0.4028,  0.4480,  0.4933,  ...,  0.6592,  0.6290,  0.5838],\n",
       "           [ 0.3877,  0.4480,  0.4933,  ...,  0.6743,  0.6441,  0.6139],\n",
       "           [ 0.3877,  0.4329,  0.4631,  ...,  0.6743,  0.6441,  0.6290]],\n",
       " \n",
       "          [[-1.7239, -1.7239, -1.7239,  ..., -0.6681, -1.4223, -1.8295],\n",
       "           [-1.7239, -1.7239, -1.7239,  ..., -0.7284, -1.2413, -1.8597],\n",
       "           [-1.7239, -1.7239, -1.7239,  ..., -1.0754, -0.6983, -1.6485],\n",
       "           ...,\n",
       "           [ 0.4028,  0.4480,  0.4933,  ...,  0.6592,  0.6290,  0.5838],\n",
       "           [ 0.3877,  0.4480,  0.4933,  ...,  0.6743,  0.6441,  0.6139],\n",
       "           [ 0.3877,  0.4329,  0.4631,  ...,  0.6743,  0.6441,  0.6290]]]]),\n",
       " ('                                 FINAL REPORT\\n EXAMINATION:  CHEST (PA AND LAT)\\n \\n INDICATION:  ___F with new onset ascites  // eval for infection\\n \\n TECHNIQUE:  Chest PA and lateral\\n \\n COMPARISON:  None.\\n \\n FINDINGS: \\n \\n There is no focal consolidation, pleural effusion or pneumothorax.  Bilateral\\n nodular opacities that most likely represent nipple shadows. The\\n cardiomediastinal silhouette is normal.  Clips project over the left lung,\\n potentially within the breast. The imaged upper abdomen is unremarkable.\\n Chronic deformity of the posterior left sixth and seventh ribs are noted.\\n \\n IMPRESSION: \\n \\n No acute cardiopulmonary process.\\n',\n",
       "  '                                 FINAL REPORT\\n EXAMINATION:  CHEST (PA AND LAT)\\n \\n INDICATION:  History: ___F with shortness of breath\\n \\n TECHNIQUE:  Chest PA and lateral\\n \\n COMPARISON:  ___\\n \\n FINDINGS: \\n \\n The cardiac, mediastinal and hilar contours are normal. Pulmonary vasculature\\n is normal.  Lungs are clear. No pleural effusion or pneumothorax is present.\\n Multiple clips are again seen projecting over the left breast.  Remote\\n left-sided rib fractures are also re- demonstrated.\\n \\n IMPRESSION: \\n \\n No acute cardiopulmonary abnormality.\\n',\n",
       "  '                                 FINAL REPORT\\n STUDY:  PA and lateral chest x-ray.\\n \\n COMPARISON EXAM:  PA and lateral chest x-ray, ___.\\n \\n INDICATION:  ___-year-old woman with left supraclavicular fullness for several\\n months with history of right lung carcinoid tumor.\\n \\n FINDINGS:  Heart size is normal.  Mediastinal contours are normal with mild\\n aortic tortuosity.  Post-surgical changes in the right hemithorax are stable\\n including thickening of the pleura along the costal surface and blunting of\\n the costophrenic sulcus.  The right sixth rib surgical fracture is\\n redemonstrated.  There are no new lung nodules identified. \\n \\n IMPRESSION:  Stable chest radiograph.\\n',\n",
       "  '                                 FINAL REPORT\\n REASON FOR EXAMINATION:  Evaluation of the patient with history of carcinoid\\n with intermittent dyspnea on exertion.\\n \\n PA and lateral upright chest radiographs were reviewed in comparison to ___.\\n \\n Heart size is normal.  Mediastinum is normal.  The post-surgical changes in\\n the right hemithorax are stable including thickening of the pleura along the\\n costal surface and blunting of the costophrenic sulcus.  The surgical fracture\\n of the right sixth rib is redemonstrated.  No new abnormalities are\\n demonstrated within the limitations of the chest radiograph technique.  Lung\\n volumes are preserved.\\n',\n",
       "  '                                 FINAL REPORT\\n EXAMINATION:  CHEST (PA AND LAT)\\n \\n INDICATION:  ___ year old woman with productive cough and bilateral rales on\\n exam.  // r/o CHF or pneumonia      PRODUCTIVE COUGH R/O CHF OR PNEUMONIA\\n \\n IMPRESSION: \\n \\n Compared to chest radiographs since ___, most recently one ___.\\n \\n Previous mild pulmonary edema and possible concurrent pneumonia has all\\n cleared.  Heart is top-normal size, improved, and pleural effusions have\\n resolved.  Right hilar vessels are still enlarged, perhaps due to pulmonary\\n arterial hypertension.  Lateral view shows atherosclerotic coronary\\n calcification in the left circumflex.\\n',\n",
       "  '                                 FINAL REPORT\\n EXAMINATION:  CHEST (PA AND LAT)\\n \\n INDICATION:  History: ___F with dyspnea\\n \\n TECHNIQUE:  Chest PA and lateral\\n \\n COMPARISON:  ___\\n \\n FINDINGS: \\n \\n Heart size remains mild to moderately enlarged.  The aorta is tortuous and\\n diffusely calcified.  Mediastinal and hilar contours are otherwise unchanged. \\n Previous pattern of mild pulmonary edema has essentially resolved.  Mild\\n atelectasis is seen in the lung bases without focal consolidation.  Blunting\\n of the costophrenic angles bilaterally suggests trace bilateral pleural\\n effusions, not substantially changed in the interval.  No pneumothorax is\\n present.\\n \\n IMPRESSION: \\n \\n Interval resolution of previously seen mild pulmonary edema with trace\\n bilateral pleural effusions.\\n',\n",
       "  '                                 FINAL REPORT\\n EXAMINATION:  CHEST (PA AND LAT)\\n \\n INDICATION:  ___ year old woman with CKD with increased dyspnea and cough x 2\\n weeks.  // R/o pulmonary edema or infiltrate      R/o pulmonary edema or\\n infiltrate\\n \\n IMPRESSION: \\n \\n As compared to ___, the lung volumes have slightly decreased.  Signs\\n of mild overinflation and moderate pleural effusions persist.  Moderate\\n cardiomegaly.  Elongation of the descending aorta.  No pneumonia.\\n',\n",
       "  '                                 FINAL REPORT\\n INDICATION:  Shortness of breath.\\n \\n COMPARISONS:  ___, ___.\\n \\n FINDINGS: PA and lateral views of the chest demonstrate low lung volumes. \\n Tiny bilateral pleural effusions are new since ___.  No signs of\\n pneumonia or pulmonary vascular congestion.  Heart is top normal in size\\n though this is stable. Aorta is markedly tortuous, unchanged.  Aortic arch\\n calcifications are seen.  There is no pneumothorax.  No focal consolidation. \\n Partially imaged upper abdomen is unremarkable.\\n \\n IMPRESSION: Tiny pleural effusions, new. Otherwise unremarkable.\\n')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da315b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = meta_transformer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9972d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 14])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca0ee913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 14])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_transformer(x[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58ea3e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 14])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c590ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Meta_Transformer(\n",
       "  (image_embedding): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (meta_encoder): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (global_avg_pooling): AdaptiveAvgPool1d(output_size=1)\n",
       "  (classification_head): Linear(in_features=768, out_features=14, bias=True)\n",
       "  (text_encoder): CLIP(\n",
       "    (visual): VisionTransformer(\n",
       "      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): Sequential(\n",
       "          (0): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (token_embedding): Embedding(49408, 512)\n",
       "    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6e448eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2733, -0.1022,  1.7166,  1.5149,  0.9412,  0.1355,  1.5457, -1.8708,\n",
       "         -0.4450, -0.0345,  0.0456, -0.8384,  0.1511, -2.0948],\n",
       "        [-1.3504, -0.0524,  1.6573,  1.3680,  1.0730,  0.0771,  1.4828, -1.6735,\n",
       "         -0.3572, -0.1226,  0.3301, -0.9064,  0.0337, -2.1048],\n",
       "        [-0.9777, -0.2089,  1.5738,  1.4654,  1.0402, -0.3487,  1.6059, -1.5584,\n",
       "         -0.2000,  0.1424,  0.0825, -0.5257, -0.1704, -2.0210],\n",
       "        [-1.0008, -0.2901,  1.6252,  1.5551,  1.0797, -0.3242,  1.5591, -1.5426,\n",
       "         -0.2182,  0.1646,  0.1738, -0.5412, -0.2003, -1.9446],\n",
       "        [-1.0343, -0.2409,  1.7178,  1.6498,  1.0568, -0.3487,  1.6970, -1.6052,\n",
       "         -0.2835,  0.1888,  0.1938, -0.5744, -0.1763, -1.9482],\n",
       "        [-0.8761, -0.3927,  1.4990,  1.4338,  1.0742, -0.4382,  1.4491, -1.4982,\n",
       "         -0.1571,  0.1150,  0.1875, -0.4960, -0.1933, -1.9997],\n",
       "        [-1.2394, -0.3136,  1.6743,  1.5161,  1.0368, -0.0418,  1.5208, -1.8409,\n",
       "         -0.2033,  0.2230,  0.3110, -0.6427,  0.1328, -1.9451],\n",
       "        [-0.8396, -0.4099,  1.4754,  1.4873,  1.1139, -0.5186,  1.5009, -1.4448,\n",
       "         -0.1510,  0.0982,  0.1610, -0.4400, -0.2509, -2.0973]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfb38c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7188, -0.4577,  1.6785,  0.8650,  0.9708,  0.8193,  1.2848, -2.9963,\n",
       "         -0.7747, -0.9468,  0.2731, -1.8168, -0.3471, -2.3613],\n",
       "        [-1.7156, -0.4407,  1.7851,  0.8381,  1.0102,  0.7418,  1.2789, -2.9622,\n",
       "         -0.8063, -0.9077,  0.2275, -1.8123, -0.3934, -2.3239],\n",
       "        [-1.8598, -0.6031,  1.7094,  0.6077,  1.2172,  0.7321,  1.1970, -3.0493,\n",
       "         -0.6644, -0.6273,  0.3871, -1.5912, -0.3445, -2.4219],\n",
       "        [-1.7303, -0.5009,  1.8373,  0.6384,  1.0309,  0.7276,  1.2096, -3.0642,\n",
       "         -0.7292, -0.7264,  0.3220, -1.7644, -0.2355, -2.4804],\n",
       "        [-1.7863, -0.4844,  1.7237,  0.7541,  1.0498,  0.7168,  1.1746, -2.9206,\n",
       "         -0.6966, -0.7722,  0.3209, -1.7049, -0.4518, -2.4405],\n",
       "        [-1.8125, -0.5053,  1.7951,  0.7229,  0.9733,  0.9157,  1.2940, -3.0284,\n",
       "         -0.7660, -0.8660,  0.1919, -1.7711, -0.3461, -2.3082],\n",
       "        [-1.8208, -0.4244,  1.7446,  0.7321,  1.0113,  0.7784,  1.0949, -3.0116,\n",
       "         -0.8293, -0.8676,  0.1778, -1.7494, -0.4086, -2.3884],\n",
       "        [-1.7916, -0.4479,  1.8737,  0.8790,  0.9302,  0.8972,  1.3124, -2.9754,\n",
       "         -0.8678, -1.0562, -0.1115, -1.9258, -0.3414, -2.2282]],\n",
       "       device='cuda:0', grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71cd8bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39141f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd95abe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2187, 0.4745, 0.8477, 0.8198, 0.7193, 0.5338, 0.8243, 0.1335, 0.3905,\n",
       "         0.4914, 0.5114, 0.3019, 0.5377, 0.1096],\n",
       "        [0.2058, 0.4869, 0.8399, 0.7971, 0.7452, 0.5193, 0.8150, 0.1580, 0.4116,\n",
       "         0.4694, 0.5818, 0.2877, 0.5084, 0.1086],\n",
       "        [0.2733, 0.4480, 0.8283, 0.8124, 0.7389, 0.4137, 0.8328, 0.1739, 0.4502,\n",
       "         0.5355, 0.5206, 0.3715, 0.4575, 0.1170],\n",
       "        [0.2688, 0.4280, 0.8355, 0.8256, 0.7464, 0.4197, 0.8262, 0.1762, 0.4457,\n",
       "         0.5411, 0.5433, 0.3679, 0.4501, 0.1251],\n",
       "        [0.2622, 0.4401, 0.8478, 0.8389, 0.7421, 0.4137, 0.8451, 0.1673, 0.4296,\n",
       "         0.5470, 0.5483, 0.3602, 0.4560, 0.1248],\n",
       "        [0.2940, 0.4031, 0.8174, 0.8075, 0.7454, 0.3922, 0.8099, 0.1827, 0.4608,\n",
       "         0.5287, 0.5467, 0.3785, 0.4518, 0.1192],\n",
       "        [0.2245, 0.4222, 0.8421, 0.8200, 0.7382, 0.4896, 0.8207, 0.1369, 0.4493,\n",
       "         0.5555, 0.5771, 0.3446, 0.5332, 0.1251],\n",
       "        [0.3016, 0.3989, 0.8139, 0.8157, 0.7528, 0.3732, 0.8177, 0.1908, 0.4623,\n",
       "         0.5245, 0.5402, 0.3917, 0.4376, 0.1094]], device='cuda:0',\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54c61e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1520, 0.3875, 0.8427, 0.7037, 0.7253, 0.6941, 0.7833, 0.0476, 0.3155,\n",
       "         0.2795, 0.5679, 0.1398, 0.4141, 0.0862],\n",
       "        [0.1524, 0.3916, 0.8563, 0.6981, 0.7331, 0.6774, 0.7823, 0.0492, 0.3087,\n",
       "         0.2875, 0.5566, 0.1404, 0.4029, 0.0892],\n",
       "        [0.1347, 0.3536, 0.8468, 0.6474, 0.7716, 0.6753, 0.7680, 0.0452, 0.3398,\n",
       "         0.3481, 0.5956, 0.1692, 0.4147, 0.0815],\n",
       "        [0.1505, 0.3773, 0.8626, 0.6544, 0.7371, 0.6743, 0.7702, 0.0446, 0.3254,\n",
       "         0.3260, 0.5798, 0.1462, 0.4414, 0.0772],\n",
       "        [0.1435, 0.3812, 0.8486, 0.6801, 0.7407, 0.6719, 0.7640, 0.0511, 0.3326,\n",
       "         0.3160, 0.5795, 0.1538, 0.3889, 0.0801],\n",
       "        [0.1403, 0.3763, 0.8576, 0.6732, 0.7258, 0.7142, 0.7848, 0.0462, 0.3173,\n",
       "         0.2961, 0.5478, 0.1454, 0.4143, 0.0904],\n",
       "        [0.1393, 0.3955, 0.8513, 0.6753, 0.7333, 0.6853, 0.7493, 0.0469, 0.3038,\n",
       "         0.2957, 0.5443, 0.1481, 0.3992, 0.0841],\n",
       "        [0.1429, 0.3899, 0.8669, 0.7066, 0.7171, 0.7104, 0.7879, 0.0485, 0.2957,\n",
       "         0.2580, 0.4721, 0.1272, 0.4155, 0.0972]], device='cuda:0',\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd46e8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1854, 0.4310, 0.8452, 0.7617, 0.7223, 0.6140, 0.8038, 0.0905, 0.3530,\n",
       "         0.3855, 0.5396, 0.2208, 0.4759, 0.0979],\n",
       "        [0.1791, 0.4392, 0.8481, 0.7476, 0.7391, 0.5983, 0.7986, 0.1036, 0.3602,\n",
       "         0.3784, 0.5692, 0.2140, 0.4557, 0.0989],\n",
       "        [0.2040, 0.4008, 0.8375, 0.7299, 0.7552, 0.5445, 0.8004, 0.1096, 0.3950,\n",
       "         0.4418, 0.5581, 0.2704, 0.4361, 0.0993],\n",
       "        [0.2097, 0.4027, 0.8491, 0.7400, 0.7418, 0.5470, 0.7982, 0.1104, 0.3855,\n",
       "         0.4335, 0.5616, 0.2571, 0.4457, 0.1012],\n",
       "        [0.2029, 0.4106, 0.8482, 0.7595, 0.7414, 0.5428, 0.8046, 0.1092, 0.3811,\n",
       "         0.4315, 0.5639, 0.2570, 0.4225, 0.1024],\n",
       "        [0.2172, 0.3897, 0.8375, 0.7404, 0.7356, 0.5532, 0.7973, 0.1144, 0.3891,\n",
       "         0.4124, 0.5473, 0.2619, 0.4331, 0.1048],\n",
       "        [0.1819, 0.4089, 0.8467, 0.7476, 0.7358, 0.5874, 0.7850, 0.0919, 0.3766,\n",
       "         0.4256, 0.5607, 0.2464, 0.4662, 0.1046],\n",
       "        [0.2222, 0.3944, 0.8404, 0.7611, 0.7350, 0.5418, 0.8028, 0.1197, 0.3790,\n",
       "         0.3913, 0.5062, 0.2595, 0.4265, 0.1033]], device='cuda:0',\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sigmoid(a) + sigmoid(b))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77d853e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0561f8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "densenet121.ra_in1k\n",
      "densenet121.tv_in1k\n",
      "densenet161.tv_in1k\n",
      "densenet169.tv_in1k\n",
      "densenet201.tv_in1k\n",
      "densenetblur121d.ra_in1k\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "# Get a list of all available models\n",
    "model_names = timm.list_models(pretrained=True)\n",
    "\n",
    "# Filter the model names to only include DenseNet models\n",
    "densenet_models = [name for name in model_names if 'densenet' in name.lower()]\n",
    "\n",
    "# Print the list of DenseNet models\n",
    "for model in densenet_models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc88fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta_test",
   "language": "python",
   "name": "meta_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
